{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This code is based on the treelstm code\n",
    "# https://github.com/tensorflow/fold/blob/master/tensorflow_fold/g3doc/sentiment.ipynb\n",
    "import codecs\n",
    "import os\n",
    "import zipfile\n",
    "import time\n",
    "import datetime\n",
    "from gensim import models\n",
    "from VocabProcessor import VocabProcessor\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "from nltk.tokenize import sexpr\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "import tensorflow_fold as td\n",
    "import pickle\n",
    "import sys\n",
    "sys.setrecursionlimit(2000)\n",
    "\n",
    "# Model Hyperparameters\n",
    "# ==================================================\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 200, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_integer(\"LEARNING_RATE\", 0.0008, \"load the previous one, number is the batch order\")\n",
    "tf.flags.DEFINE_integer(\"KEEP_PROB\", 0.75, \"load the previous one, number is the batch order\")\n",
    "tf.flags.DEFINE_integer(\"BATCH_SIZE\", 100, \"load the previous one, number is the batch order\")\n",
    "tf.flags.DEFINE_integer(\"EPOCHS\", 100, \"load the previous one, number is the batch order\")\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "\n",
    "# pre-trained word embedding is trained with separate words.\n",
    "# Original code was taken & modified from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "# ==================================================\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\"  \", \" \", string)\n",
    "    return string.lower()\n",
    "\n",
    "def separateFeatures(string):\n",
    "    for line in string:\n",
    "        line = clean_str(line)\n",
    "        pid = line.split(\"\\t\")[0]\n",
    "        sen = line.split(\"\\t\")[1]\n",
    "        ddiCheck = line.split(\"\\t\")[2]\n",
    "        ddiType = line.split(\"\\t\")[3]\n",
    "        drug1 = line.split(\"\\t\")[4]\n",
    "        drug1Name = line.split(\"\\t\")[5]\n",
    "        drug1Type = line.split(\"\\t\")[6]\n",
    "        drug2 = line.split(\"\\t\")[7]\n",
    "        drug2Name = line.split(\"\\t\")[8]\n",
    "        drug2Type = line.split(\"\\t\")[9]\n",
    "        binaryParsedTree = line.split(\"\\t\")[10].strip()\n",
    "        parsedWholeSen = line.split(\"\\t\")[11]\n",
    "        \n",
    "        yield binaryParsedTree, parsedWholeSen\n",
    "\n",
    "# Loads DDI challenge'13 data from files\n",
    "# The data should be preprocessed before.\n",
    "# ==================================================\n",
    "def load_data_and_labels(string):\n",
    "    samples = list(open(string, \"r\").readlines())\n",
    "    return list(separateFeatures(samples))\n",
    "\n",
    "data_dir = \"./runs/testModel\"\n",
    "print('saving model files to %s' % data_dir)\n",
    "TrainFeatures = load_data_and_labels(\"data/DDItrain_recur\")\n",
    "TestFeatures = load_data_and_labels(\"data/DDItest_recur\")\n",
    "\n",
    "train_trees = [Tf[0] for Tf in TrainFeatures]\n",
    "test_trees = [Tf[0] for Tf in TestFeatures]\n",
    "\n",
    "#shuffle the training trees\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(TrainFeatures)))\n",
    "train_trees = np.array(train_trees)[shuffle_indices]\n",
    "\n",
    "print(len(train_trees))\n",
    "print(len(test_trees))\n",
    "\n",
    "#this part is necessary for making a vocabulary\n",
    "allSens1 = [Tf[1] for Tf in TrainFeatures] + [Tf[1] for Tf in TestFeatures]\n",
    "splitted1 = [sentence.split(\", \") for sentence in allSens1]\n",
    "max_document_length1 = max(len(s) for s in splitted1)\n",
    "vocab_proc1 = VocabProcessor(max_document_length1, tokenizer_fn=\"splitComma\")\n",
    "np.array(list(vocab_proc1.fit_transform([Tf[1] for Tf in TrainFeatures])))\n",
    "np.array(list(vocab_proc1.fit_transform([Tf[1] for Tf in TestFeatures])))\n",
    "vocab_proc1.vocabulary_.freeze()\n",
    "\n",
    "#Since original word embedding is too large, the training time takes too much time.\n",
    "#We only use the words appeared in the DDI'13 corpus only.\n",
    "#We have saved the selected WE words using pickle\n",
    "ft1 = open('shorten_ensemble/pubpmc.pickle', 'rb')\n",
    "embedding_for_given_index1 = pickle.load(ft1)\n",
    "ft1.close()\n",
    "\n",
    "print(len(embedding_for_given_index1))\n",
    "   \n",
    "weight_matrix = embedding_for_given_index1\n",
    "word_idx = vocab_proc1.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#    This model is based on the model of 'Improved Semantic\n",
    "#    Representations From Tree-Structured Long Short-Term Memory\n",
    "#    Networks' <http://arxiv.org/pdf/1503.00075.pdf>, with recurrent\n",
    "#    dropout as described in 'Recurrent Dropout without Memory Loss'\n",
    "#    <http://arxiv.org/pdf/1603.05118.pdf>.   \n",
    "#    Originally, this code is based on the tensorflow fold library.\n",
    "#\n",
    "#=====================\n",
    "class BinaryTreeLSTMCell(tf.contrib.rnn.RNNCell):\n",
    "#   num_units: int, The number of units in the LSTM cell.\n",
    "#   keep_prob: Keep probability for recurrent dropout.    \n",
    "#=====================\n",
    "    def __init__(self, num_units, keep_prob=1.0):\n",
    "        super(BinaryTreeLSTMCell, self).__init__()\n",
    "#   init process        \n",
    "        self._keep_prob = keep_prob\n",
    "        self._num_units = num_units\n",
    "        self.state_size = (num_units, num_units)\n",
    "        self.output_size = num_units * 1\n",
    "    def state_size(self):\n",
    "        self.state_size = (self._num_units, self._num_units)\n",
    "    def output_size(self):\n",
    "        self.output_size = (self._num_units * 1)\n",
    "\n",
    "    def __call__(self, inputs, state, contextVec, ent1Vec, ent2Vec, scope=None):\n",
    "        with tf.variable_scope(scope or type(self).__name__):\n",
    "            inputs = tf.nn.dropout(inputs, self._keep_prob)\n",
    "            lhs, rhs = state\n",
    "            c_0, h_0 = lhs#cell and hidden states from left child\n",
    "            c_1, h_1 = rhs#cell and hidden states from right child\n",
    "            \n",
    "            concat0 = tf.contrib.layers.fully_connected(\n",
    "              tf.concat([contextVec, ent1Vec, ent2Vec, inputs, h_0, h_1], 1), 5 * self._num_units, trainable=True)\n",
    "            \n",
    "            # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "            i_0, j_0, f_00, f_01, o_0 = tf.split(value=concat0, num_or_size_splits=5, axis=1)\n",
    "            j_0 = tf.tanh(j_0)\n",
    "            if not isinstance(self._keep_prob, float) or self._keep_prob < 1:\n",
    "                j_0 = tf.nn.dropout(j_0, self._keep_prob)\n",
    "                \n",
    "            new_c0 = (c_0 * tf.sigmoid(f_00 + 1.0) +\n",
    "                      c_1 * tf.sigmoid(f_01 + 1.0) +\n",
    "                      tf.sigmoid(i_0) * j_0)\n",
    "            new_h0 = tf.tanh(new_c0) * tf.sigmoid(o_0)\n",
    "                        \n",
    "            resultH = tf.concat([new_h0], 1)\n",
    "            resultH = tf.nn.dropout(resultH, self._keep_prob)\n",
    "            return resultH, [new_c0, new_h0]\n",
    "#dropout keep probability, with a default of 1 (for eval).\n",
    "keep_prob_ph = tf.placeholder_with_default(1.0, [])\n",
    "keep_prob_ph = tf.placeholder_with_default(1.0, [])\n",
    "\n",
    "lstm_num_units = 128   # Tai et al. used 150\n",
    "tree_lstm = td.ScopedLayer(\n",
    "    BinaryTreeLSTMCell(lstm_num_units, keep_prob=keep_prob_ph),\n",
    "    name_or_scope='tree_lstm')\n",
    "NUM_CLASSES = 2  # number of classes\n",
    "#fully connected layer\n",
    "output_layer = td.FC(NUM_CLASSES, activation=None, name='output_layer')\n",
    "\n",
    "#word embedding for DDI'13 corpus should be false. fine-tuning raise the overfitting issue\n",
    "word_embedding = td.Embedding(\n",
    "    *weight_matrix.shape, initializer=weight_matrix, name='word_embedding', trainable = False)\n",
    "\n",
    "#declare recursive model\n",
    "embed_subtree = td.ForwardDeclaration(name='embed_subtree')\n",
    "\n",
    "#convert subtree containment (context) feature to vector\n",
    "def makeContextMat(input1):\n",
    "    input1 = int(input1)\n",
    "    if input1 == 0:\n",
    "        return [1 for i in range(10)]\n",
    "    else:\n",
    "        return [0 for i in range(10)]\n",
    "\n",
    "#convert position feature to vector\n",
    "def makeEntPositMat(givenInput):\n",
    "    position_embed = [[1,1,1,1,1,1,1,1,1,1],\n",
    "                 [1,0,1,1,1,1,1,1,1,1],\n",
    "                 [1,0,0,1,1,1,1,1,1,1],\n",
    "                 [1,0,0,0,1,1,1,1,1,1],\n",
    "                 [1,0,0,0,0,1,1,1,1,1],\n",
    "                 [1,0,0,0,0,0,1,1,1,1],\n",
    "                 [1,0,0,0,0,0,0,1,1,1],\n",
    "                 [1,0,0,0,0,0,0,0,1,1],\n",
    "                 [1,0,0,0,0,0,0,0,0,1],\n",
    "                 [0,0,0,0,0,0,0,0,0,0],\n",
    "                 [0,0,0,0,0,0,0,0,0,1],\n",
    "                 [0,0,0,0,0,0,0,0,1,1],\n",
    "                 [0,0,0,0,0,0,0,1,1,1],\n",
    "                 [0,0,0,0,0,0,1,1,1,1],\n",
    "                 [0,0,0,0,0,1,1,1,1,1],\n",
    "                 [0,0,0,0,1,1,1,1,1,1],\n",
    "                 [0,0,0,1,1,1,1,1,1,1],\n",
    "                 [0,0,1,1,1,1,1,1,1,1],\n",
    "                 [0,1,1,1,1,1,1,1,1,1]]\n",
    "    intInput = int(givenInput)\n",
    "    return position_embed[intInput]\n",
    "\n",
    "def logits_and_state():\n",
    "    \"\"\"Creates a block that goes from tokens to (logits, state) tuples.\"\"\"\n",
    "    unknown_idx = len(word_idx)\n",
    "    \n",
    "    lookup_word = lambda word: word_idx.get(word)#unknown_idx is the default return value\n",
    "    word2vec = (td.GetItem(0) >> td.GetItem(0) >> td.InputTransform(lookup_word) >>\n",
    "                td.Scalar('int32') >> word_embedding)#<td.Pipe>: None -> TensorType((200,), 'float32')\n",
    "    \n",
    "    #make a copy of vectors for the leaf node or the internal node computation\n",
    "    context2vec1 = td.GetItem(1) >> td.InputTransform(makeContextMat) >> td.Vector(10)\n",
    "    context2vec2 = td.GetItem(1) >> td.InputTransform(makeContextMat) >> td.Vector(10)\n",
    "    ent1posit1 = td.GetItem(2) >> td.InputTransform(makeEntPositMat) >> td.Vector(10)\n",
    "    ent1posit2 = td.GetItem(2) >> td.InputTransform(makeEntPositMat) >> td.Vector(10)\n",
    "    ent2posit1 = td.GetItem(3) >> td.InputTransform(makeEntPositMat) >> td.Vector(10)\n",
    "    ent2posit2 = td.GetItem(3) >> td.InputTransform(makeEntPositMat) >> td.Vector(10)\n",
    "    \n",
    "    pair2vec = td.GetItem(0) >> (embed_subtree(), embed_subtree())\n",
    "    # Trees are binary, so the tree layer takes two states as its input_state.\n",
    "    zero_state = td.Zeros((tree_lstm.state_size,) * 2)\n",
    "    # zero initialized\n",
    "    zero_inp = td.Zeros(word_embedding.output_type.shape[0])#word_embedding.output_type.shape[0] == 200\n",
    "    \n",
    "    word_case = td.AllOf(word2vec, zero_state, context2vec1, ent1posit1, ent2posit1)\n",
    "    pair_case = td.AllOf(zero_inp, pair2vec, context2vec2, ent1posit2, ent2posit2)\n",
    "    #if leaf case, go to word case\n",
    "    tree2vec = td.OneOf(lambda pair: len(pair[0]), [(1, word_case), (2, pair_case)])\n",
    "    \n",
    "    return tree2vec >> tree_lstm >> (output_layer, td.Identity())#logits and lstm result states\n",
    "\n",
    "#Define a per-node loss function for training.\n",
    "def tf_node_loss(logits, labels):\n",
    "    # Ensures that the loss for examples whose ground truth class is `1` is 2x\n",
    "    # higher than the loss for negative instances.\n",
    "    # when test data do not have a labels, just give 0 to all test labels.\n",
    "    # and calculate the results with logits.\n",
    "    weight = tf.multiply(2.0, tf.cast(tf.equal(labels, 1), tf.float32)) + 1\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "    losses = losses*weight\n",
    "    return losses\n",
    "\n",
    "def tf_hits(logits, labels):\n",
    "    predictions = tf.cast(tf.argmax(logits, 1), tf.int32)\n",
    "    return tf.cast(tf.equal(predictions, labels), tf.float32)\n",
    "def tf_pred(logits):\n",
    "    return tf.cast(tf.argmax(logits, 1), tf.int32)\n",
    "def tf_logits(logits):\n",
    "    return logits\n",
    "def tf_label(labels):\n",
    "    return labels\n",
    "\n",
    "def add_metrics(is_root):\n",
    "    c = td.Composition(\n",
    "      name='predict(is_root=%s)' % (is_root))\n",
    "    with c.scope():\n",
    "        labels = c.input[0]\n",
    "        logits = td.GetItem(0).reads(c.input[1])        \n",
    "        state = td.GetItem(1).reads(c.input[1])\n",
    "\n",
    "        loss = td.Function(tf_node_loss)\n",
    "        td.Metric('all_loss').reads(loss.reads(logits, labels))#do not need the loss of all nodes\n",
    "        if is_root: td.Metric('root_loss').reads(loss)\n",
    "\n",
    "        #save logits for ensemble\n",
    "        result_logits = td.Function(tf_logits)\n",
    "        td.Metric('all_logits').reads(result_logits.reads(logits))\n",
    "        if is_root: td.Metric('root_logits').reads(result_logits)\n",
    "        #save pred and labels for validation\n",
    "        pred = td.Function(tf_pred)\n",
    "        td.Metric('all_pred').reads(pred.reads(logits))\n",
    "        if is_root: td.Metric('root_pred').reads(pred)\n",
    "        answer = td.Function(tf_label)\n",
    "        td.Metric('all_labels').reads(answer.reads(labels))\n",
    "        if is_root: td.Metric('root_label').reads(answer)\n",
    "\n",
    "        c.output.reads(state)\n",
    "    return c\n",
    "\n",
    "#separate labels, features and contents\n",
    "def tokenize(s):\n",
    "    labelAndFeatures, treeContent = s[1:-1].split(None, 1)\n",
    "    label, outerContext, ent1Posit, ent2Posit = labelAndFeatures.split(\"/\")\n",
    "    # detection\n",
    "    if label == '0':\n",
    "        return '0', (sexpr.sexpr_tokenize(treeContent), outerContext, ent1Posit, ent2Posit)\n",
    "    else:\n",
    "        return '1', (sexpr.sexpr_tokenize(treeContent), outerContext, ent1Posit, ent2Posit)\n",
    "    #classification\n",
    "#     return label, (sexpr.sexpr_tokenize(treeContent), outerContext, ent1Posit, ent2Posit)\n",
    "\n",
    "def embed_tree(is_root):\n",
    "    return td.InputTransform(tokenize) >> (td.Scalar('int32'), logits_and_state()) >> add_metrics(is_root)\n",
    "\n",
    "model = embed_tree(is_root=True)\n",
    "#Resolve the forward declaration for embedding subtrees (the non-root case) with a second call to embed_tree.\n",
    "embed_subtree.resolve_to(embed_tree(is_root=False))\n",
    "#print('input type: %s' % model.input_type)\n",
    "#print('output type: %s' % model.output_type)\n",
    "compiler = td.Compiler.create(model)\n",
    "#build model end\n",
    "#=================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred = compiler.metric_tensors['root_pred']   #validation\n",
    "labels = compiler.metric_tensors['root_label']#validation\n",
    "result_logits = compiler.metric_tensors['root_logits']#ensemble\n",
    "\n",
    "train_feed_dict = {keep_prob_ph: FLAGS.KEEP_PROB}\n",
    "loss = tf.reduce_sum(compiler.metric_tensors['root_loss'])#only root loss is calculated\n",
    "opt = tf.train.AdamOptimizer(FLAGS.LEARNING_RATE)#adam optimizer is effective\n",
    "\n",
    "grads_and_vars = opt.compute_gradients(loss)\n",
    "train_op = opt.apply_gradients(grads_and_vars)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def train_step(batch):\n",
    "    train_feed_dict[compiler.loom_input_tensor] = batch\n",
    "    _, batch_loss = sess.run([train_op, loss], train_feed_dict)\n",
    "    return batch_loss\n",
    "\n",
    "def train_epoch(train_set_shuffled):\n",
    "    return sum(train_step(batch) for batch in td.group_by_batches(train_set_shuffled, FLAGS.BATCH_SIZE))\n",
    "\n",
    "train_set = compiler.build_loom_inputs(train_trees)\n",
    "test_feed_dict = compiler.build_feed_dict(test_trees)\n",
    "\n",
    "#Run the main training loop, save the model after designated epoch\n",
    "save_path = os.path.join(data_dir, 'DDI_finding_model')\n",
    "for epoch, shuffled in enumerate(td.epochs(train_set, FLAGS.EPOCHS), 1):\n",
    "    train_loss = train_epoch(shuffled)\n",
    "    print(\"epoch %s finished. train_loss : %s\" % (epoch, train_loss))    \n",
    "    if epoch == 100:\n",
    "        checkpoint_path = saver.save(sess, save_path, global_step=epoch)\n",
    "        print('model saved in file: %s' % checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#test the model that we saved above (single model)\n",
    "saver.restore(sess, \"runs/testModel/DDI_finding_model-100\")\n",
    "_test_loss = tf.reduce_sum(compiler.metric_tensors['root_loss'])\n",
    "_test_logits = compiler.metric_tensors['root_logits']\n",
    "test_loss, test_pred, test_labels, test_logits = sess.run([_test_loss, pred, labels, _test_logits],\n",
    "                                             test_feed_dict)\n",
    "\n",
    "#rough result test\n",
    "#Note that this should not be the final result.\n",
    "#Remember that in the preprocessing phase, 8 positive instances are filtered as negative.\n",
    "#Among the 8 positive instance, 4 are the int type, 1 is the mechanism type, and 3 are the effect type.\n",
    "#We report the exact score with our own f1_score calculation.\n",
    "f1score = metrics.f1_score(test_labels, test_pred, average=None)\n",
    "precision = metrics.precision_score(test_labels, test_pred, average=None)\n",
    "recall = metrics.recall_score(test_labels, test_pred, average=None)\n",
    "print('!!test!! : test_loss_avg: %.3e, test_f1score: [%s]\\n, test_prec : [%s], test_recall : [%s]'\n",
    "      % (test_loss, f1score, precision, recall))\n",
    "\n",
    "#save the logits for later, (e.g. result calculation, ensemble)\n",
    "fpred = open(\"runs/testModel/logits_1\", \"w\")\n",
    "for i in range(len(test_logits)):\n",
    "    fpred.write(str(test_logits[i]))\n",
    "    fpred.write(\"\\n\")\n",
    "fpred.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
