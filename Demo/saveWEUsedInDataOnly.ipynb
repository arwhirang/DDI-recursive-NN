{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import zipfile\n",
    "import time\n",
    "import datetime\n",
    "from gensim import models\n",
    "#VocabProcessor is taken from the tensorflow.contrib learn.preprocessing.VocabularyProcessor and modified by us.\n",
    "from VocabProcessor import VocabProcessor\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "from nltk.tokenize import sexpr\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "import tensorflow_fold as td\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "# Model Hyperparameters\n",
    "# ==================================================\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 200, \"Dimensionality of character embedding (default: 128)\")\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "\n",
    "# pre-trained word embedding is trained with separate words.\n",
    "# Original code was taken & modified from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "# ==================================================\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\"  \", \" \", string)\n",
    "    return string.lower()\n",
    "\n",
    "def separateFeatures(string):\n",
    "    for line in string:\n",
    "        line = clean_str(line)\n",
    "        pid = line.split(\"\\t\")[0]\n",
    "        sen = line.split(\"\\t\")[1]\n",
    "        ddiCheck = line.split(\"\\t\")[2]\n",
    "        ddiType = line.split(\"\\t\")[3]\n",
    "        drug1 = line.split(\"\\t\")[4]\n",
    "        drug1Name = line.split(\"\\t\")[5]\n",
    "        drug1Type = line.split(\"\\t\")[6]\n",
    "        drug2 = line.split(\"\\t\")[7]\n",
    "        drug2Name = line.split(\"\\t\")[8]\n",
    "        drug2Type = line.split(\"\\t\")[9]\n",
    "        binaryParsedTree = line.split(\"\\t\")[10].strip()\n",
    "        parsedWholeSen = line.split(\"\\t\")[11]\n",
    "        \n",
    "        yield binaryParsedTree, parsedWholeSen\n",
    "\n",
    "# Loads DDI challenge'13 data from files\n",
    "# The data should be preprocessed before.\n",
    "# ==================================================\n",
    "def load_data_and_labels(string):\n",
    "    samples = list(open(string, \"r\").readlines())\n",
    "    return list(separateFeatures(samples))\n",
    "\n",
    "#detection\n",
    "TrainFeatures = load_data_and_labels(\"data/DDItrain_verRP\")\n",
    "TestFeatures = load_data_and_labels(\"data/DDItest_verRP\")\n",
    "\n",
    "#this part is necessary for making a vocabulary\n",
    "allSens1 = [Tf[1] for Tf in TrainFeatures] + [Tf[1] for Tf in TestFeatures]\n",
    "splitted1 = [sentence.split(\", \") for sentence in allSens1]\n",
    "max_document_length1 = max(len(s) for s in splitted1)\n",
    "vocab_proc1 = VocabProcessor(max_document_length1, tokenizer_fn=\"splitComma\")\n",
    "np.array(list(vocab_proc1.fit_transform([Tf[1] for Tf in TrainFeatures])))\n",
    "np.array(list(vocab_proc1.fit_transform([Tf[1] for Tf in TestFeatures])))\n",
    "vocab_proc1.vocabulary_.freeze()\n",
    "\n",
    "print(len(vocab_proc1.vocabulary_._mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#load Word Embedding\n",
    "bioEmbed = models.Word2Vec.load_word2vec_format('bioWordEmbedding/PubMed-and-PMC-w2v.bin', binary=True)\n",
    "word_for_given_index1 = {}\n",
    "for strWord in vocab_proc1.vocabulary_._mapping:\n",
    "    word_for_given_index1[vocab_proc1.vocabulary_.get(strWord)] = strWord\n",
    "count = 0\n",
    "\n",
    "#initialize the whole word's vector with random numbers\n",
    "#and if WE contains a word, replace the word's vector with the vector in WE.\n",
    "init_range = math.sqrt(6.0 / (1 + 200))\n",
    "embedding_for_given_index1 = np.random.uniform(-init_range,init_range,(len(vocab_proc1.vocabulary_), FLAGS.embedding_dim)).astype(np.float32)\n",
    "for strIndex in range(len(word_for_given_index1)):\n",
    "    strWord = word_for_given_index1.get(strIndex)\n",
    "    if strWord in bioEmbed:\n",
    "        embedding_for_given_index1[count] = bioEmbed[strWord]\n",
    "    count = count + 1\n",
    "\n",
    "#If we use the raw WE, the training process takes a lot of time\n",
    "#                          due to the enormous size of vectors,\n",
    "#we use the words appeared in the DDI corpus only to speed up the training.\n",
    "#we save the selected WE words using pickle\n",
    "ft = open('shorten_ensemble/pubpmc.pickle', 'wb')\n",
    "pickle.dump(embedding_for_given_index1, ft)\n",
    "ft.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
